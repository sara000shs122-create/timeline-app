# timeline-app
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

st.set_page_config(page_title="Timeline + RandomForest Threat Predictor", layout="wide")

st.title("ğŸ›¡ï¸ Threat-Level Predictor + Interactive Timeline")
st.write(
    "Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ ÙŠØ·Ø¨Ù‘Ù‚ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© **Random Forest** Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙ‡Ø¯ÙŠØ¯ØŒ "
    "ÙŠØ³ØªØ®Ø¯Ù… **NumPy** Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ø¹Ø·ÙŠØ§ØªØŒ Ùˆ**Matplotlib** Ù„Ø±Ø³Ù… Ø§Ù„Ø®Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ ÙˆØ§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©."
)

# Sidebar - data upload or generate sample
st.sidebar.header("Ø¨ÙŠØ§Ù†Ø§Øª")
data_source = st.sidebar.radio("Ø§Ø®ØªØ± Ù…ØµØ¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:", ("Upload CSV", "Generate Sample"))

if data_source == "Upload CSV":
    uploaded = st.sidebar.file_uploader("Ø§Ø±ÙØ¹ Ù…Ù„Ù CSV ÙŠØ­ØªÙˆÙŠ ØµÙÙˆÙ Ø¨ÙŠØ§Ù†Ø§Øª (features) ÙˆØ¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù 'threat_level' Ø£Ùˆ Ø§Ø®ØªØ± Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø¨Ù†ÙØ³Ùƒ", type=["csv"])
    if uploaded is not None:
        df = pd.read_csv(uploaded)
    else:
        df = None
else:
    # Generate sample dataset
    st.sidebar.info("ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙˆØ§Ø±ÙŠØ®ØŒ Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙŠØ²Ø§ØªØŒ ÙˆÙ…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙ‡Ø¯ÙŠØ¯ (0=low,1=medium,2=high).")
    n = st.sidebar.slider("Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ (Ø¹ÙŠÙ†Ø©)", 100, 5000, 800)
    rng = np.random.default_rng(42)
    dates = pd.date_range(end=pd.Timestamp.today(), periods=n).to_series().sample(frac=1, random_state=42).reset_index(drop=True)
    feature_1 = rng.normal(loc=0.0, scale=1.0, size=n) + (np.linspace(-1,1,n))
    feature_2 = rng.integers(0, 100, size=n)
    feature_3 = rng.normal(loc=5.0, scale=2.0, size=n)
    # create correlated target
    scores = 0.3*feature_1 + 0.02*feature_2 + 0.1*feature_3 + rng.normal(0,0.5,n)
    thresholds = np.quantile(scores, [0.33, 0.66])
    threat_level = np.digitize(scores, thresholds)
    df = pd.DataFrame({
        "date": dates,
        "feature_1": feature_1,
        "feature_2": feature_2,
        "feature_3": feature_3,
        "threat_level": threat_level
    })
    df = df.sample(frac=1, random_state=1).reset_index(drop=True)

if df is None:
    st.info("Ø§Ù†ØªØ¸Ø±ÙŠ Ø±ÙØ¹ Ù…Ù„Ù CSV Ø£Ùˆ Ø§Ø®ØªØ§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ.")
    st.stop()

st.subheader("Ù…Ø¹Ø§ÙŠÙ†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
st.dataframe(df.head())

# Select columns
all_columns = df.columns.tolist()
st.sidebar.subheader("Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
target_col = st.sidebar.selectbox("Ø§Ø®ØªØ± Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù (target):", options=all_columns, index=all_columns.index("threat_level") if "threat_level" in all_columns else 0)
date_col = st.sidebar.selectbox("Ø§Ø®ØªØ± Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØ§Ø±ÙŠØ® (Ø§Ø®ØªÙŠØ§Ø±ÙŠ):", options=[None] + all_columns, index=1 if "date" in all_columns else 0)
# Features selected automatically as numeric columns except target and date
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
default_features = [c for c in numeric_cols if c != target_col]
features = st.sidebar.multiselect("Ø§Ø®ØªØ± Ù…ÙŠØ²Ø§Øª (features) Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:", options=default_features, default=default_features)

if len(features) == 0:
    st.error("Ù…Ù‡Ù…: Ø§Ø®ØªØ§Ø±ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ù…ÙŠØ²Ø© ÙˆØ§Ø­Ø¯Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.")
    st.stop()

# Preprocessing
st.subheader("Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… NumPy")
st.write("Ù†Ù‚ÙˆÙ… Ø¨ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ§Øª NumPy Ù„Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©.")

# Drop rows where target is missing
before_rows = df.shape[0]
df_clean = df.dropna(subset=[target_col])
after_rows = df_clean.shape[0]
st.write(f"Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ: {before_rows} â†’ Ø¨Ø¹Ø¯ Ø­Ø°Ù Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© ÙÙŠ Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‡Ø¯Ù: {after_rows}")

# For selected features, fill missing with median
for col in features:
    if df_clean[col].isnull().any():
        median = df_clean[col].median()
        df_clean[col] = df_clean[col].fillna(median)

# Convert feature matrix and target vector to NumPy arrays
X = df_clean[features].to_numpy(dtype=float)
y = df_clean[target_col].to_numpy(dtype=int)

st.write(f"Ø´ÙƒÙ„ Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª X: {X.shape} â€” Ø´ÙƒÙ„ Ø§Ù„Ù…ØªØ¬Ù‡ Ø§Ù„Ù‡Ø¯Ù y: {y.shape}")

# Train / Test split
test_size = st.sidebar.slider("Ù†Ø³Ø¨Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± (test size)", 5, 50, 20) / 100.0
random_state = st.sidebar.number_input("Random state (Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±)", min_value=1, max_value=9999, value=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=int(random_state), stratify=y if len(np.unique(y))>1 else None)

st.subheader("ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Random Forest")
n_estimators = st.sidebar.slider("Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø´Ø¬Ø§Ø± (n_estimators)", 10, 500, 100)
max_depth = st.sidebar.slider("Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¹Ù…Ù‚ Ø§Ù„Ø´Ø¬Ø±Ø© (max_depth) â€” 0 ÙŠØ¹Ù†ÙŠ Ø¨Ø¯ÙˆÙ† Ø­Ø¯", 0, 50, 0)
max_depth_val = None if max_depth == 0 else int(max_depth)

clf = RandomForestClassifier(n_estimators=int(n_estimators), max_depth=max_depth_val, random_state=int(random_state))
with st.spinner("Ø¬Ø§Ø±Ù ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬..."):
    clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
acc = accuracy_score(y_test, y_pred)
st.success(f"ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨! Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {acc:.4f}")

st.subheader("ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡")
st.text("Classification report:")
st.text(classification_report(y_test, y_pred, digits=4))

cm = confusion_matrix(y_test, y_pred)
st.write("Confusion Matrix:")
st.write(pd.DataFrame(cm, index=np.unique(y), columns=np.unique(y)))

# Feature importance
st.subheader("Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª (Feature Importance)")
importances = clf.feature_importances_
fi_df = pd.DataFrame({"feature": features, "importance": importances}).sort_values("importance", ascending=False)
st.table(fi_df)

# Matplotlib visualizations - must use plt (no seaborn)
st.subheader("Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ© ÙˆØ§Ù„Ù€ Timeline Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Matplotlib")

# 1) Feature importance bar chart
fig1, ax1 = plt.subplots(figsize=(6,3))
ax1.bar(fi_df["feature"], fi_df["importance"])
ax1.set_title("Feature Importances")
ax1.set_ylabel("Importance")
ax1.set_xlabel("Feature")
plt.xticks(rotation=30)
st.pyplot(fig1)

# 2) Confusion matrix heatmap (matplotlib)
fig2, ax2 = plt.subplots(figsize=(5,4))
cax = ax2.matshow(cm, cmap='viridis')
fig2.colorbar(cax)
ax2.set_title("Confusion Matrix")
ax2.set_xlabel("Predicted")
ax2.set_ylabel("Actual")
ax2.set_xticks(range(len(np.unique(y))))
ax2.set_yticks(range(len(np.unique(y))))
st.pyplot(fig2)

# 3) Timeline plot: requires a date column selected
if date_col and date_col in df_clean.columns:
    st.subheader("Ø§Ù„Ø®Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ â€” Ù…ØªÙˆØ³Ø· Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙ‡Ø¯ÙŠØ¯ Ø¹Ø¨Ø± Ø§Ù„Ø²Ù…Ù†")
    # ensure date column is datetime
    dates = pd.to_datetime(df_clean[date_col])
    timeline_df = pd.DataFrame({
        "date": dates,
        "threat": df_clean[target_col]
    })
    timeline_df = timeline_df.sort_values("date")
    timeline_df.set_index("date", inplace=True)
    daily_mean = timeline_df["threat"].resample("D").mean().fillna(method="ffill").fillna(0)
    fig3, ax3 = plt.subplots(figsize=(10,3))
    ax3.plot(daily_mean.index, daily_mean.values, marker='o', linewidth=1)
    ax3.set_title("Ù…ØªÙˆØ³Ø· Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙ‡Ø¯ÙŠØ¯ â€” ÙŠÙˆÙ…ÙŠØ§Ù‹")
    ax3.set_ylabel("Ù…ØªÙˆØ³Ø· ØªÙ‡Ø¯ÙŠØ¯")
    ax3.set_xlabel("Ø§Ù„ØªØ§Ø±ÙŠØ®")
    plt.xticks(rotation=25)
    st.pyplot(fig3)
else:
    st.info("Ù„Ù… ØªØ­Ø¯Ø¯ÙŠ Ø¹Ù…ÙˆØ¯ Ø§Ù„ØªØ§Ø±ÙŠØ®ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø±Ø³Ù… Ø§Ù„Ø®Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ. ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù…ÙˆØ¯ ØªØ§Ø±ÙŠØ® ÙˆØ§Ø®ØªÙŠØ§Ø±Ù‡ Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ.")

# Allow user to run predictions on new rows
st.subheader("ØªØ¬Ø±Ø¨Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©")
st.write("Ø£Ø¯Ø®Ù„ÙŠ Ù‚ÙŠÙ… Ù…ÙŠØ²Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø³ÙŠØªÙ… Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙ‡Ø¯ÙŠØ¯).")
input_vals = {}
cols = st.columns(len(features))
for i, f in enumerate(features):
    with cols[i]:
        input_vals[f] = st.number_input(f"Ù‚ÙŠÙ…Ø© {f}", value=float(df_clean[f].median()))
input_arr = np.array([input_vals[f] for f in features], dtype=float).reshape(1, -1)
pred = clf.predict(input_arr)[0]
st.write(f"Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙ‡Ø¯ÙŠØ¯ Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª: **{pred}**")

# Offer download of model results (predictions on whole dataset)
st.subheader("ØªÙ†Ø²ÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
if st.button("Ø­Ø³Ù‘Ù† Ø§Ù„ØªÙ†Ø¨ÙˆØ¡Ø§Øª ÙˆØ£Ø­ÙØ¸ CSV"):
    preds_all = clf.predict(df_clean[features].to_numpy())
    out_df = df_clean.copy()
    out_df["predicted_threat"] = preds_all
    csv = out_df.to_csv(index=False).encode('utf-8')
    st.download_button("ØªØ­Ù…ÙŠÙ„ CSV Ù…Ø¹ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª", data=csv, file_name="predictions.csv", mime="text/csv")

st.markdown("---")
st.caption("ØªÙ… Ø§Ù„ØªØ·ÙˆÙŠØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RandomForest (scikit-learn)ØŒ NumPy Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©ØŒ ÙˆMatplotlib Ù„Ù„Ø±Ø³ÙˆÙ…. ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ù…Ù„ÙÙƒ Ø§Ù„Ø®Ø§Øµ Ø£Ùˆ ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø¹ÙŠÙ†Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©.")
